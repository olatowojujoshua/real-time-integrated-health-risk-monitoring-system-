{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372ad16e-6e9d-435d-908b-d26f057e4b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"resp_health_db\"\n",
    "spark.sql(f\"USE {SCHEMA_NAME}\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aac0d1e-a5a1-41e5-bf01-7e8af2068f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "virus_daily_sql = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW virus_daily AS\n",
    "SELECT\n",
    "  report_date AS date,\n",
    "  province,\n",
    "  -- average positivity across viruses where metric_type looks like a percentage\n",
    "  AVG(CASE\n",
    "        WHEN LOWER(metric_type) LIKE '%percent%' \n",
    "          OR LOWER(metric_type) LIKE '%positiv%'\n",
    "        THEN metric_value\n",
    "      END) AS avg_positivity,\n",
    "  -- sum of other clinical metrics (outbreaks, hospitalizations, etc.)\n",
    "  SUM(CASE\n",
    "        WHEN NOT (LOWER(metric_type) LIKE '%percent%' \n",
    "               OR LOWER(metric_type) LIKE '%positiv%')\n",
    "        THEN COALESCE(metric_value, 0)\n",
    "      END) AS total_clinical_count\n",
    "FROM respiratory_activity\n",
    "GROUP BY report_date, province\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(virus_daily_sql)\n",
    "spark.sql(\"SELECT * FROM virus_daily LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773a1bdd-4e75-4fd2-94b1-c1f9810888d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_daily_sql = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW weather_daily AS\n",
    "SELECT\n",
    "  DATE(timestamp) AS date,\n",
    "  province,\n",
    "  AVG(temperature_c)    AS avg_temp,\n",
    "  AVG(wind_chill_c)     AS avg_wind_chill,\n",
    "  AVG(humidity_percent) AS avg_humidity\n",
    "FROM weather_conditions\n",
    "GROUP BY DATE(timestamp), province\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(weather_daily_sql)\n",
    "spark.sql(\"SELECT * FROM weather_daily LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c483fa-0fbd-440e-a550-44d2876ecf37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "virus_df   = spark.table(\"virus_daily\")\n",
    "weather_df = spark.table(\"weather_daily\")\n",
    "\n",
    "# Join on date + province\n",
    "joined = (\n",
    "    virus_df.alias(\"v\")\n",
    "    .join(weather_df.alias(\"w\"), on=[\"date\", \"province\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "# ----- VIRUS RISK -----\n",
    "# Combine positivity + clinical counts into a single intensity measure\n",
    "virus_base = joined.withColumn(\n",
    "    \"virus_intensity\",\n",
    "    F.coalesce(F.col(\"avg_positivity\"), F.lit(0.0)) +\n",
    "    F.coalesce(F.col(\"total_clinical_count\"), F.lit(0.0))\n",
    ")\n",
    "\n",
    "w_prov = Window.partitionBy(\"province\")\n",
    "\n",
    "virus_scaled = (\n",
    "    virus_base\n",
    "    .withColumn(\"virus_min\", F.min(\"virus_intensity\").over(w_prov))\n",
    "    .withColumn(\"virus_max\", F.max(\"virus_intensity\").over(w_prov))\n",
    "    .withColumn(\n",
    "        \"virus_risk_score\",\n",
    "        F.when(F.col(\"virus_max\") == F.col(\"virus_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"virus_intensity\") - F.col(\"virus_min\")) /\n",
    "             (F.col(\"virus_max\")   - F.col(\"virus_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----- COLD RISK -----\n",
    "cold_base = virus_scaled.withColumn(\n",
    "    \"cold_index\",\n",
    "    F.when(F.col(\"avg_wind_chill\").isNotNull(), -F.col(\"avg_wind_chill\"))\n",
    "     .otherwise(-F.col(\"avg_temp\"))\n",
    ")\n",
    "\n",
    "cold_scaled = (\n",
    "    cold_base\n",
    "    .withColumn(\"cold_min\", F.min(\"cold_index\").over(w_prov))\n",
    "    .withColumn(\"cold_max\", F.max(\"cold_index\").over(w_prov))\n",
    "    .withColumn(\n",
    "        \"cold_risk_score\",\n",
    "        F.when(F.col(\"cold_max\") == F.col(\"cold_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"cold_index\") - F.col(\"cold_min\")) /\n",
    "             (F.col(\"cold_max\")   - F.col(\"cold_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----- Combined risk -----\n",
    "scored = cold_scaled.withColumn(\n",
    "    \"combined_risk_score\",\n",
    "    (F.col(\"virus_risk_score\") + F.col(\"cold_risk_score\")) / 2.0\n",
    ")\n",
    "\n",
    "scored.select(\"date\",\"province\",\"virus_risk_score\",\"cold_risk_score\",\"combined_risk_score\")\\\n",
    "      .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23baef3e-26a0-456f-9080-639677d1a1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE resp_health_db\")\n",
    "\n",
    "virus_df   = spark.table(\"virus_daily\")\n",
    "weather_df = spark.table(\"weather_daily\")\n",
    "\n",
    "print(\"virus_daily rows:\", virus_df.count())\n",
    "print(\"weather_daily rows:\", weather_df.count())\n",
    "\n",
    "print(\"\\nvirus_daily date range:\")\n",
    "virus_df.select(\n",
    "    F.min(\"date\").alias(\"min_date\"),\n",
    "    F.max(\"date\").alias(\"max_date\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nweather_daily date range:\")\n",
    "weather_df.select(\n",
    "    F.min(\"date\").alias(\"min_date\"),\n",
    "    F.max(\"date\").alias(\"max_date\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nvirus_daily provinces:\")\n",
    "virus_df.select(\"province\").distinct().orderBy(\"province\").show(truncate=False)\n",
    "\n",
    "print(\"\\nweather_daily provinces:\")\n",
    "weather_df.select(\"province\").distinct().orderBy(\"province\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a845290-8955-46e7-bc1c-808247507f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark.sql(\"USE resp_health_db\")\n",
    "\n",
    "virus_df = spark.table(\"virus_daily\")\n",
    "\n",
    "virus_df.show(10, truncate=False)\n",
    "print(\"virus_daily rows:\", virus_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2bbd83-2c39-46b1-8eb6-a868c9b61a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "virus_intensity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816f4641-a44d-405b-a454-679387376426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine positivity and clinical counts\n",
    "virus_base = virus_df.withColumn(\n",
    "    \"virus_intensity\",\n",
    "    F.coalesce(F.col(\"avg_positivity\"), F.lit(0.0)) +\n",
    "    F.coalesce(F.col(\"total_clinical_count\"), F.lit(0.0))\n",
    ")\n",
    "\n",
    "# Since province is just 'Canada', we can scale across the whole table\n",
    "w_all = Window.orderBy(\"date\")\n",
    "\n",
    "min_val = virus_base.agg(F.min(\"virus_intensity\").alias(\"min\")).collect()[0][\"min\"]\n",
    "max_val = virus_base.agg(F.max(\"virus_intensity\").alias(\"max\")).collect()[0][\"max\"]\n",
    "\n",
    "print(\"Intensity min/max:\", min_val, max_val)\n",
    "\n",
    "virus_scaled = virus_base.withColumn(\n",
    "    \"virus_risk_score\",\n",
    "    F.when(F.lit(max_val) == F.lit(min_val), F.lit(0.0))\n",
    "     .otherwise(\n",
    "         (F.col(\"virus_intensity\") - F.lit(min_val)) /\n",
    "         (F.lit(max_val) - F.lit(min_val)) * 100.0\n",
    "     )\n",
    ")\n",
    "\n",
    "virus_scaled.select(\"date\",\"province\",\"avg_positivity\",\"total_clinical_count\",\"virus_risk_score\")\\\n",
    "            .orderBy(\"date\")\\\n",
    "            .show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e8a231-64cc-4ce8-9638-09e5e58610d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w_all = Window.partitionBy(\"province\").orderBy(\"date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9e244a-7c0f-4989-972e-bfe2a2dceaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "combined scores (virus-only for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b9654f-8252-4172-89d2-9b52a47607f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "risk_scores_df = (\n",
    "    virus_scaled\n",
    "    .withColumn(\"cold_risk_score\", F.lit(None).cast(\"double\"))     # no weather yet\n",
    "    .withColumn(\"combined_risk_score\", F.col(\"virus_risk_score\"))  # same as virus\n",
    "    .withColumn(\n",
    "        \"risk_category\",\n",
    "        F.when(F.col(\"combined_risk_score\") < 25,  \"Low\")\n",
    "         .when(F.col(\"combined_risk_score\") < 50, \"Moderate\")\n",
    "         .when(F.col(\"combined_risk_score\") < 75, \"High\")\n",
    "         .otherwise(\"Very High\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add id + created_at to match your existing table structure\n",
    "risk_scores_df = (\n",
    "    risk_scores_df\n",
    "    .withColumn(\"id\", F.row_number().over(w_all).cast(\"long\"))\n",
    "    .withColumn(\"created_at\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"id\",\n",
    "        \"date\",\n",
    "        \"province\",\n",
    "        \"virus_risk_score\",\n",
    "        \"cold_risk_score\",\n",
    "        \"combined_risk_score\",\n",
    "        \"risk_category\",\n",
    "        \"created_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "risk_scores_df.show(20, truncate=False)\n",
    "print(\"Rows:\", risk_scores_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a27055-cb58-4d75-abb5-0d17d2d8bc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "risk_scores_df.show(10, truncate=False)\n",
    "print(\"Rows:\", risk_scores_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3434e7c-d129-4623-86aa-a5c97c66082a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE resp_health_db\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS risk_scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b398a877-3f17-4ee1-9774-1be051044d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061f2874-b249-4ee1-b6f2-e025c3c9b17d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "risk_scores_df.write.mode(\"overwrite\").saveAsTable(\"risk_scores\")\n",
    "\n",
    "spark.table(\"risk_scores\").show(10, truncate=False)\n",
    "print(\"Final count:\", spark.table(\"risk_scores\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459d44e6-72d0-45ba-aa85-1dd1085ae632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"risk_scores\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9af9f5-73a7-46ef-9c51-c5d447df62ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install Meteostat in this cluster\n",
    "%pip install meteostat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9983139d-0e56-42f4-aa79-3daa80ebede2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from meteostat import Stations, Daily\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE resp_health_db\")\n",
    "\n",
    "# Use the same date range as virus_daily\n",
    "start = datetime(2025, 8, 30)\n",
    "end   = datetime(2025, 11, 29)\n",
    "\n",
    "# 1) Find Canadian weather stations with daily data in that period\n",
    "stations = (\n",
    "    Stations()\n",
    "    .region('CA')              # Canada\n",
    "    .inventory('daily', (start, end))\n",
    ")\n",
    "\n",
    "stations_df = stations.fetch(10)  # get top 10 stations\n",
    "print(stations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf01093-1e0f-4f32-9916-1e79972e8cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pick the first 5 stations for national average\n",
    "station_ids = stations_df.index[:5].tolist()\n",
    "station_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30442872-4e5d-4d4a-b7ab-3f4d14e0be75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch daily data for each station and combine\n",
    "data_list = []\n",
    "\n",
    "for sid in station_ids:\n",
    "    d = Daily(sid, start, end).fetch()\n",
    "    d[\"station\"] = sid\n",
    "    data_list.append(d)\n",
    "\n",
    "weather_pd = pd.concat(data_list)\n",
    "\n",
    "# Aggregate to national daily average\n",
    "weather_pd = (\n",
    "    weather_pd\n",
    "    .groupby(\"time\")\n",
    "    .agg({\n",
    "        \"tavg\": \"mean\",   # average temperature\n",
    "        \"tmin\": \"mean\",\n",
    "        \"tmax\": \"mean\",\n",
    "        \"wspd\": \"mean\"    # wind speed\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(weather_pd.head())\n",
    "print(weather_pd.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6fd745-a54a-4232-b25e-6649185f15c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert pandas -> Spark\n",
    "weather_sdf = spark.createDataFrame(weather_pd)\n",
    "\n",
    "# Rename 'time' to 'date' and add national province label\n",
    "weather_sdf = (\n",
    "    weather_sdf\n",
    "    .withColumnRenamed(\"time\", \"date\")\n",
    "    .withColumn(\"province\", F.lit(\"Canada\"))\n",
    ")\n",
    "\n",
    "weather_sdf.createOrReplaceTempView(\"weather_daily_meteostat\")\n",
    "\n",
    "weather_sdf.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a046fdce-e751-48de-bc31-56ec766a801e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_sdf.select(F.min(\"date\"), F.max(\"date\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25263566-827a-4e29-93b0-80460cdc4e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "virus_df   = spark.table(\"virus_daily\")             # has date, province='Canada', avg_positivity, total_clinical_count\n",
    "weather_df = spark.table(\"weather_daily_meteostat\") # date, tavg, tmin, tmax, wspd, province='Canada'\n",
    "\n",
    "# Join on date + province\n",
    "joined = (\n",
    "    virus_df.alias(\"v\")\n",
    "    .join(weather_df.alias(\"w\"), on=[\"date\", \"province\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "print(\"Joined rows:\", joined.count())\n",
    "joined.select(\"date\", \"province\", \"avg_positivity\", \"total_clinical_count\", \"tavg\", \"wspd\")\\\n",
    "      .orderBy(\"date\")\\\n",
    "      .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f17359d-504e-477d-98b4-ed5647ff7fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "virus_base = joined.withColumn(\n",
    "    \"virus_intensity\",\n",
    "    F.coalesce(F.col(\"avg_positivity\"), F.lit(0.0)) +\n",
    "    F.coalesce(F.col(\"total_clinical_count\"), F.lit(0.0))\n",
    ")\n",
    "\n",
    "w_canada = Window.partitionBy(\"province\")\n",
    "\n",
    "virus_scaled = (\n",
    "    virus_base\n",
    "    .withColumn(\"virus_min\", F.min(\"virus_intensity\").over(w_canada))\n",
    "    .withColumn(\"virus_max\", F.max(\"virus_intensity\").over(w_canada))\n",
    "    .withColumn(\n",
    "        \"virus_risk_score\",\n",
    "        F.when(F.col(\"virus_max\") == F.col(\"virus_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"virus_intensity\") - F.col(\"virus_min\")) /\n",
    "             (F.col(\"virus_max\")   - F.col(\"virus_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50c1184-da47-44bd-b7a2-5f4b21aad9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cold_base = virus_scaled.withColumn(\n",
    "    \"cold_index\",\n",
    "    -F.col(\"tavg\")   # colder (lower tavg) => higher index\n",
    ")\n",
    "\n",
    "cold_scaled = (\n",
    "    cold_base\n",
    "    .withColumn(\"cold_min\", F.min(\"cold_index\").over(w_canada))\n",
    "    .withColumn(\"cold_max\", F.max(\"cold_index\").over(w_canada))\n",
    "    .withColumn(\n",
    "        \"cold_risk_score\",\n",
    "        F.when(F.col(\"cold_max\") == F.col(\"cold_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"cold_index\") - F.col(\"cold_min\")) /\n",
    "             (F.col(\"cold_max\")   - F.col(\"cold_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57242c0-e5ea-4f47-9d53-89f1ac606375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored = cold_scaled.withColumn(\n",
    "    \"combined_risk_score\",\n",
    "    0.6 * F.col(\"virus_risk_score\") + 0.4 * F.col(\"cold_risk_score\")\n",
    ")\n",
    "\n",
    "scored = scored.withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"combined_risk_score\") < 25,  \"Low\")\n",
    "     .when(F.col(\"combined_risk_score\") < 50, \"Moderate\")\n",
    "     .when(F.col(\"combined_risk_score\") < 75, \"High\")\n",
    "     .otherwise(\"Very High\")\n",
    ")\n",
    "\n",
    "w_all = Window.partitionBy(\"province\").orderBy(\"date\")\n",
    "\n",
    "risk_scores_df = (\n",
    "    scored\n",
    "    .select(\n",
    "        \"date\",\n",
    "        \"province\",\n",
    "        \"virus_risk_score\",\n",
    "        \"cold_risk_score\",\n",
    "        \"combined_risk_score\",\n",
    "        \"risk_category\"\n",
    "    )\n",
    "    .withColumn(\"id\", F.row_number().over(w_all).cast(\"long\"))\n",
    "    .withColumn(\"created_at\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"id\",\n",
    "        \"date\",\n",
    "        \"province\",\n",
    "        \"virus_risk_score\",\n",
    "        \"cold_risk_score\",\n",
    "        \"combined_risk_score\",\n",
    "        \"risk_category\",\n",
    "        \"created_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "risk_scores_df.orderBy(\"date\").show(20, truncate=False)\n",
    "print(\"Rows:\", risk_scores_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167fec06-74a1-4cbd-b922-0bd856f827ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS risk_scores\")\n",
    "risk_scores_df.write.mode(\"overwrite\").saveAsTable(\"risk_scores\")\n",
    "\n",
    "spark.table(\"risk_scores\").show(10, truncate=False)\n",
    "print(\"Final count:\", spark.table(\"risk_scores\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc1eee7-fbbd-48c8-9e2f-beb56ca88944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(risk_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0942c85-f6f5-458d-bddf-d90c9696b25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clinical_raw = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/FileStore/clinical/Clinical data - 2025-12-05.csv\")\n",
    ")\n",
    "\n",
    "lab_raw = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/FileStore/lab/Laboratory data - 2025-12-05.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd9f532-2ed4-4618-998a-8b7155066296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clinical_raw.show(5)\n",
    "lab_raw.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-12-06 18_18_30",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
