{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66611da6-eb78-4ed1-ace2-1e9302f1297f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"resp_health_db\"\n",
    "spark.sql(f\"USE {SCHEMA_NAME}\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbaf637-17d9-436a-ab30-a919b8062252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "virus_daily_sql = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW virus_daily AS\n",
    "SELECT\n",
    "  report_date AS date,\n",
    "  province,\n",
    "  -- average percent positive across viruses\n",
    "  AVG(CASE\n",
    "        WHEN metric_type IN ('percent_positive', 'positivity_rate')\n",
    "        THEN metric_value\n",
    "      END) AS avg_positivity,\n",
    "  -- total clinical counts (e.g. outbreaks, hospitalizations)\n",
    "  SUM(CASE\n",
    "        WHEN metric_type NOT IN ('percent_positive', 'positivity_rate')\n",
    "        THEN COALESCE(metric_value, 0)\n",
    "      END) AS total_clinical_count\n",
    "FROM respiratory_activity\n",
    "GROUP BY report_date, province\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(virus_daily_sql)\n",
    "spark.sql(\"SELECT * FROM virus_daily LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50982333-d202-49aa-9639-83a09ba5e088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_daily_sql = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW weather_daily AS\n",
    "SELECT\n",
    "  DATE(timestamp) AS date,\n",
    "  province,\n",
    "  AVG(temperature_c)      AS avg_temp,\n",
    "  AVG(wind_chill_c)       AS avg_wind_chill,\n",
    "  AVG(humidity_percent)   AS avg_humidity\n",
    "FROM weather_conditions\n",
    "GROUP BY DATE(timestamp), province\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(weather_daily_sql)\n",
    "spark.sql(\"SELECT * FROM weather_daily LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0495b6c2-0359-43be-9cd6-d51ae7504a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "join and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f056d55-044a-4800-a2a9-c7febbf1fddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "virus_df   = spark.table(\"virus_daily\")\n",
    "weather_df = spark.table(\"weather_daily\")\n",
    "\n",
    "# Inner join on date + province (you can change to left join if you want)\n",
    "joined = (\n",
    "    virus_df.alias(\"v\")\n",
    "    .join(\n",
    "        weather_df.alias(\"w\"),\n",
    "        on=[\"date\", \"province\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- VIRUS RISK: combine positivity + clinical counts, then min-max scale per province ----\n",
    "virus_base = (\n",
    "    joined\n",
    "    .withColumn(\n",
    "        \"virus_intensity\",\n",
    "        F.coalesce(F.col(\"avg_positivity\"), F.lit(0.0)) +\n",
    "        F.coalesce(F.col(\"total_clinical_count\"), F.lit(0.0))\n",
    "    )\n",
    ")\n",
    "\n",
    "w_prov = Window.partitionBy(\"province\")\n",
    "\n",
    "virus_scaled = (\n",
    "    virus_base\n",
    "    .withColumn(\"virus_min\", F.min(\"virus_intensity\").over(w_prov))\n",
    "    .withColumn(\"virus_max\", F.max(\"virus_intensity\").over(w_prov))\n",
    "    .withColumn(\n",
    "        \"virus_risk_score\",\n",
    "        F.when(F.col(\"virus_max\") == F.col(\"virus_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"virus_intensity\") - F.col(\"virus_min\")) /\n",
    "             (F.col(\"virus_max\") - F.col(\"virus_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- COLD RISK: colder = higher risk; scale using (0 - temperature/windchill) ----\n",
    "cold_base = virus_scaled.withColumn(\n",
    "    \"cold_index\",\n",
    "    F.when(F.col(\"avg_wind_chill\").isNotNull(), -F.col(\"avg_wind_chill\"))\n",
    "     .otherwise(-F.col(\"avg_temp\"))\n",
    ")\n",
    "\n",
    "cold_scaled = (\n",
    "    cold_base\n",
    "    .withColumn(\"cold_min\", F.min(\"cold_index\").over(w_prov))\n",
    "    .withColumn(\"cold_max\", F.max(\"cold_index\").over(w_prov))\n",
    "    .withColumn(\n",
    "        \"cold_risk_score\",\n",
    "        F.when(F.col(\"cold_max\") == F.col(\"cold_min\"), 0.0)\n",
    "         .otherwise(\n",
    "             (F.col(\"cold_index\") - F.col(\"cold_min\")) /\n",
    "             (F.col(\"cold_max\") - F.col(\"cold_min\")) * 100.0\n",
    "         )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- Combined risk: simple average of virus + cold ----\n",
    "scored = cold_scaled.withColumn(\n",
    "    \"combined_risk_score\",\n",
    "    (F.col(\"virus_risk_score\") + F.col(\"cold_risk_score\")) / 2.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d505dd-c66c-416d-848f-a5b9779f0250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored = scored.withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"combined_risk_score\") < 25,  F.lit(\"Low\"))\n",
    "     .when(F.col(\"combined_risk_score\") < 50, F.lit(\"Moderate\"))\n",
    "     .when(F.col(\"combined_risk_score\") < 75, F.lit(\"High\"))\n",
    "     .otherwise(F.lit(\"Very High\"))\n",
    ")\n",
    "\n",
    "risk_scores_df = (\n",
    "    scored\n",
    "    .select(\n",
    "        F.col(\"date\"),\n",
    "        F.col(\"province\"),\n",
    "        F.col(\"virus_risk_score\"),\n",
    "        F.col(\"cold_risk_score\"),\n",
    "        F.col(\"combined_risk_score\"),\n",
    "        F.col(\"risk_category\")\n",
    "    )\n",
    "    .withColumn(\"created_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "risk_scores_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e5ffa9-b2d7-4f77-b4a3-a19963fb1bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "risk_scores_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"risk_scores\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM risk_scores LIMIT 20\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e027de-406f-41c9-badf-c1673c2dcf0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"resp_health_db\"\n",
    "spark.sql(f\"USE {SCHEMA_NAME}\")\n",
    "\n",
    "risk_scores_sdf = spark.table(\"risk_scores\")\n",
    "display(risk_scores_sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad4cd2e-c130-4bd8-8270-8ea3bd9fdd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"resp_health_db\"\n",
    "spark.sql(f\"USE {SCHEMA_NAME}\")\n",
    "\n",
    "risk_scores_sdf = spark.table(\"risk_scores\")\n",
    "risk_scores_sdf.show(5, truncate=False)\n",
    "print(\"Count:\", risk_scores_sdf.count())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_build_risk_scores",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
